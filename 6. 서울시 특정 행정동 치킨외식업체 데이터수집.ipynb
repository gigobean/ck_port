{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8481e4",
   "metadata": {},
   "source": [
    "### 6. 서울시 특정 행정동 치킨외식업체 데이터수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe053a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색 동 : 묵1동\n",
      "묵1동치킨집 103개(정확도 순) 데이터 수집\n",
      "\n",
      "묵1동 치킨집(카카오맵) 데이터 수집 시작\n",
      "묵1동 1 번째 페이지 완료\n",
      "묵1동 2 번째 페이지 완료\n",
      "묵1동 3 번째 페이지 완료\n",
      "묵1동 4 번째 페이지 완료\n",
      "묵1동 5 번째 페이지 완료\n",
      "묵1동 6 번째 페이지 완료\n",
      "묵1동 7 번째 페이지 완료\n",
      "묵1동 8 번째 페이지 완료\n",
      "묵1동 9 번째 페이지 완료\n",
      "묵1동 10 번째 페이지 완료\n",
      "묵1동 완료================\n",
      "\n",
      "\n",
      "전체 데이터 수집 완료\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup     \n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# warning ignore\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') \n",
    "# 옵션 생성\n",
    "options = webdriver.ChromeOptions()\n",
    "# 창 숨기는 옵션 추가\n",
    "options.add_argument(\"headless\")\n",
    "# 검색 지역 받기\n",
    "keyword = input(\"검색 동 : \")\n",
    "print(keyword + '치킨집 103개(정확도 순) 데이터 수집')\n",
    "print()\n",
    "print(keyword, '치킨집(카카오맵) 데이터 수집 시작')\n",
    "# driver 실행\n",
    "url=\"https://map.kakao.com/\"\n",
    "path = \"./data/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "driver.get(url)\n",
    "time.sleep(0.7)\n",
    "\n",
    "# 검색창 찾고 키워드 입력\n",
    "searchbox = driver.find_element_by_xpath(\"//input[@id='search.keyword.query']\")\n",
    "searchbox.send_keys(keyword + \" 치킨\")\n",
    "\n",
    "# 검색버튼 click\n",
    "searchbutton = driver.find_element_by_xpath(\"//button[@id='search.keyword.submit']\")\n",
    "driver.execute_script(\"arguments[0].click();\", searchbutton)\n",
    "\n",
    "# delay 2 sec\n",
    "time.sleep(2)\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 데이터 프레임 생성\n",
    "df = pd.DataFrame(columns=['업종','가게이름','평가점수','평가인원', \n",
    "                           '리뷰갯수', '주소', \"전화\", \"영업시간\",\"링크주소\"])\n",
    "\n",
    "# 페이지 데이터 추출\n",
    "crawdata= soup.find_all(\"li\",\"PlaceItem clickArea\")\n",
    "\n",
    "# 첫번째 페이지의 각 업체 데이터 파싱\n",
    "for company in range(len(crawdata)):\n",
    "\n",
    "    li= soup.find_all(\"li\",\"PlaceItem clickArea\")[company]\n",
    "    upjong = li.find(\"span\",\"subcategory clickable\").text #업종\n",
    "    shop = li.find(\"a\",\"link_name\").text               #가게이름\n",
    "    point = li.find(\"em\",\"num\").text                   #평가점수\n",
    "    person = li.find(\"a\",\"numberofscore\").text[:-1]  #몇명평가\n",
    "    reviews2 = li.find(\"a\",\"review\").text[3:]         #리뷰-블로그\n",
    "    add1 = li.find(\"div\",\"addr\")\n",
    "    add = add1.find_all(\"p\")[0].text                     #주소\n",
    "    phone1 = li.find(\"div\",\"contact clickArea\")\n",
    "    phone = phone1.find(\"span\").text                     #전화번호\n",
    "    selltime = li.find(\"div\",\"openhour\").text.replace(\"\\n\",\"\")[3:] #영업시간    \n",
    "    b_link = li.find_all(\"a\")[7].get('href')    # 해당 업체 상세페이지 링크\n",
    "\n",
    "    # 데이터 프레임에 저장\n",
    "    df=df.append({'업종':upjong,'가게이름':shop,'평가점수':point,'평가인원':person, \n",
    "                  '리뷰갯수':reviews2, '주소':add, \"전화\":phone, \"영업시간\":selltime,\"링크주소\":b_link}, ignore_index=True)\n",
    "\n",
    "print(keyword+\" 1 번째 페이지 완료\")\n",
    "\n",
    "# 더보기 버튼이 있을경우 더보기 버튼 누르기 (2번)\n",
    "try:\n",
    "    time.sleep(1)\n",
    "    element1 = driver.find_element_by_id(\"info.search.place.more\")\n",
    "    ActionChains(driver).move_to_element(element1).click(element1).perform()\n",
    "    ActionChains(driver).move_to_element(element1).click(element1).perform()\n",
    "\n",
    "    try:\n",
    "        for nextpage in range(3,6,1):\n",
    "            time.sleep(1)\n",
    "            # 2~5번 페이지 데이터 추출\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            crawdata= soup.find_all(\"li\",\"PlaceItem clickArea\")[1]\n",
    "\n",
    "            # 2~5번 페이지의 각 업체 데이터 파싱 \n",
    "            for company in range(len(crawdata)):\n",
    "                selltime=\"\"\n",
    "                li= soup.find_all(\"li\",\"PlaceItem clickArea\")[company]\n",
    "                upjong = li.find(\"span\",\"subcategory clickable\").text #업종\n",
    "                shop = li.find(\"a\",\"link_name\").text               #가게이름\n",
    "                point = li.find(\"em\",\"num\").text                   #평가점수\n",
    "                person = li.find(\"a\",\"numberofscore\").text[:-1]      #몇명평가\n",
    "                reviews2 = li.find(\"a\",\"review\").text[3:]           #리뷰-블로그\n",
    "                add1 = li.find(\"div\",\"addr\")\n",
    "                add = add1.find_all(\"p\")[0].text                     #주소\n",
    "                phone1 = li.find(\"div\",\"contact clickArea\")\n",
    "                phone = phone1.find(\"span\").text                     #전화번호\n",
    "                selltime = li.find(\"div\",\"openhour\").text.replace(\"\\n\",\"\")[3:] #영업시간\n",
    "                b_link = li.find_all(\"a\")[7].get('href')                # 해당 업체 상세페이지 링크\n",
    "\n",
    "                # 데이터프레임에 저장 붙히기\n",
    "                df=df.append({'업종':upjong,'가게이름':shop,'평가점수':point,'평가인원':person, \n",
    "                              '리뷰갯수':reviews2, '주소':add, \"전화\":phone, \"영업시간\":selltime,\"링크주소\":b_link}, ignore_index=True)\n",
    "\n",
    "            print( keyword, nextpage-1, \"번째 페이지 완료\")\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "            # 다음 페이지 버튼 누르기\n",
    "            element1 = driver.find_element_by_id(\"info.search.page.no{}\".format(nextpage))\n",
    "            ActionChains(driver).move_to_element(element1).click(element1).perform()\n",
    "    except:\n",
    "        pass\n",
    "    print(keyword+\" 5 번째 페이지 완료\")    \n",
    "\n",
    "    time.sleep(1)\n",
    "    # 5번 페이지 이후의 다음 목록 페이지 화살표 누르기\n",
    "    element1 = driver.find_element_by_id(\"info.search.page.next\")\n",
    "    ActionChains(driver).move_to_element(element1).click(element1).perform()\n",
    "\n",
    "\n",
    "    for nextpage in range(2,7,1):\n",
    "        # 6~10번 페이지 데이터 추출\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        crawdata= soup.find_all(\"li\",\"PlaceItem clickArea\")[1]\n",
    "        # 6~10번 페이지의 각 업체 데이터 파싱\n",
    "        for company in range(len(crawdata)):\n",
    "            selltime=\"\"\n",
    "            li= soup.find_all(\"li\",\"PlaceItem clickArea\")[company]\n",
    "            upjong = li.find(\"span\",\"subcategory clickable\").text #업종\n",
    "            shop = li.find(\"a\",\"link_name\").text               #가게이름\n",
    "            point = li.find(\"em\",\"num\").text                   #평가점수\n",
    "            person = li.find(\"a\",\"numberofscore\").text[:-1]    #몇명평가\n",
    "            reviews2 = li.find(\"a\",\"review\").text[3:]          #리뷰-블로그\n",
    "            add1 = li.find(\"div\",\"addr\")\n",
    "            add = add1.find_all(\"p\")[0].text                     #주소\n",
    "            phone1 = li.find(\"div\",\"contact clickArea\")\n",
    "            phone = phone1.find(\"span\").text                     #전화번호\n",
    "            selltime = li.find(\"div\",\"openhour\").text.replace(\"\\n\",\"\")[3:] #영업시간\n",
    "            b_link = li.find_all(\"a\")[7].get('href') # 해당 업체 상세페이지 링크\n",
    "\n",
    "            # 데이터 프레임에 저장 붙히기\n",
    "            df=df.append({'업종':upjong,'가게이름':shop,'평가점수':point,'평가인원':person, \n",
    "                          '리뷰갯수':reviews2, '주소':add, \"전화\":phone, \"영업시간\":selltime,\"링크주소\":b_link}, ignore_index=True)\n",
    "\n",
    "        print( keyword, nextpage+4, \"번째 페이지 완료\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        # 다음 페이지 버튼 누르기\n",
    "        element1 = driver.find_element_by_id(\"info.search.page.no{}\".format(nextpage))\n",
    "        ActionChains(driver).move_to_element(element1).click(element1).perform()\n",
    "\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 저장 위치 디렉토리 미리 생성되어있어야함\n",
    "# csv로 각 지역별로 저장\n",
    "savepath = \"./data/\"\n",
    "df.to_csv(savepath+keyword+\" 치킨집 카카오맵 데이터 크롤링.csv\", index = False, mode='w', encoding = 'utf-8-sig')\n",
    "print(keyword+' 완료'+'================')\n",
    "print()\n",
    "driver.close()\n",
    "    \n",
    "print()\n",
    "print('전체 데이터 수집 완료')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
